discriminant analysis multi no mp pk xy fk for prediction compute the decision function dk log pk log fk constant qd an muk sigma kl dan muk sigma nb independent pdf over dim of qd given lets model the dim feature by multivariate normal distribution with mean and co variance matrix kkk pp 11 kk 11 pk pppp its pdf is given by fk 12 12 exp 12 kt kt kp pl kj xj kj xl kl 11 lda if further assume all we need to compute is dk xx kt log log which is linear function of xx kt xt xt kt estimate by the pooled sample co variance matrix kkk iy ik xi xi kt 15 what if does not exist replace it by ip where is small number or compute as follows assume up 1000 20000 ut 10001 20000 ut 163 rd dim after rotation is the null space for all classes what if does not exist replace it by ip where is small number or compute as follows assume up 1000 20000 ut 10001 20000 ut 163 rd dim after rotation is the null space for all classes reduced rank lda suppose is an identity matrix ip then we can write the discriminant function for lda as dk xx 22 log the feature vector only appears in the st term which is the squared distance from tok the center of the th class two points determine saline three points determine plane class centers determine ak dim subspace next well show that we can replace the squared distance in the original dim space by square distance in ak dim subspace that is lda naturally leads to dimension reduction from tok 117 when   new coordinates make prediction input test point and output from training computes vd of ud ut and aud ut compute the projection matrix based on akb pk ak bb tb dimension reduction xh fork computed xx 22 log output arg mink dk 23 how we compute the projection of on to the centers run regression of rta design matrix formed by centers latex it sha base 64 wv bn ru 380 xc cr lt aab xi cb vd ls gn be oy nr hf qx 69 dab bu bl mack dz cz rh zap hy 948 civ hm jj nm dj hy ff xd dh ff kw fg bsg pb 39 jc km dnb 39 ow uz km tek uv ty rpm ew kr cx pox wg kf lg jy mn bb kl cc rz nap cog vf lal kcr cx if qo xb bi qi hg nj umg ij lc bts nmr jie 06 kr gg js rn 107 mv mg kf hal br or yle wzi ji mw hz llby 53 uz xh oz np kk vj xp fva py gj nie ut rz dr zn kya hrt kw qo jc cj dfx iw ti oy ml poi dcd cu cq wh xu qc jam rd mk ef ecf lr hj wp an fp iw latex it latex it sha base 64 wv bn ru 380 xc cr lt aab xi cb vd ls gn be oy nr hf qx 69 dab bu bl mack dz cz rh zap hy 948 civ hm jj nm dj hy ff xd dh ff kw fg bsg pb 39 jc km dnb 39 ow uz km tek uv ty rpm ew kr cx pox wg kf lg jy mn bb kl cc rz nap cog vf lal kcr cx if qo xb bi qi hg nj umg ij lc bts nmr jie 06 kr gg js rn 107 mv mg kf hal br or yle wzi ji mw hz llby 53 uz xh oz np kk vj xp fva py gj nie ut rz dr zn kya hrt kw qo jc cj dfx iw ti oy ml poi dcd cu cq wh xu qc jam rd mk ef ecf lr hj wp an fp iw latex it latex it sha base 64 wv bn ru 380 xc cr lt aab xi cb vd ls gn be oy nr hf qx 69 dab bu bl mack dz cz rh zap hy 948 civ hm jj nm dj hy ff xd dh ff kw fg bsg pb 39 jc km dnb 39 ow uz km tek uv ty rpm ew kr cx pox wg kf lg jy mn bb kl cc rz nap cog vf lal kcr cx if qo xb bi qi hg nj umg ij lc bts nmr jie 06 kr gg js rn 107 mv mg kf hal br or yle wzi ji mw hz llby 53 uz xh oz np kk vj xp fva py gj nie ut rz dr zn kya hrt kw qo jc cj dfx iw ti oy ml poi dcd cu cq wh xu qc jam rd mk ef ecf lr hj wp an fp iw latex it latex it sha base 64 wv bn ru 380 xc cr lt aab xi cb vd ls gn be oy nr hf qx 69 dab bu bl mack dz cz rh zap hy 948 civ hm jj nm dj hy ff xd dh ff kw fg bsg pb 39 jc km dnb 39 ow uz km tek uv ty rpm ew kr cx pox wg kf lg jy mn bb kl cc rz nap cog vf lal kcr cx if qo xb bi qi hg nj umg ij lc bts nmr jie 06 kr gg js rn 107 mv mg kf hal br or yle wzi ji mw hz llby 53 uz xh oz np kk vj xp fva py gj nie ut rz dr zn kya hrt kw qo jc cj dfx iw ti oy ml poi dcd cu cq wh xu qc jam rd mk ef ecf lr hj wp an fp iw latex it fisher discriminant analysis supervised dimension reduction find direction such that the projection of data of multiple classes on to this direction is well separated the within class and between class sample co variance matrices wp kkk iy ik xi xi kt pp kk nkk kt the following qualities are trivial nx ny ny you can view was the sample co variance matrix over xi yi same as the pooled sample co variance matrix in lda and bas the sample co variance matrix over the class centers yi 26 generalized eigenvalue problem max at baa twa max at ba sub to twa assume wu ut deneb 12 where 12 ud ut is symmetric and write its inverse as 12 ud uta tba at 12 12 12 12 aw 12 at 12 12 12 btw 12 12 sub to 21 the optimization above is classical eigenvalue problem 27 different from pca we can solve the directions sequentially as follows the tei gen vector of matrix 12 12 then solve 12 the nde gen vector of matrix 12 12 then solve 12 note that although bj are ortho normal but js are not bt bj at wa bt jbl at wal we can extract at most directions since the rank of is the directions span exactly the same space as the one from the reduced rank lda 28 connection with reduced rank lda consider simple case with en what the dim space spanned by ei gen vectors of lda vs fd alda classier fda dimension reduction method ie the output of fda is set of directions but not classic ation rule the normal assumption is never mentioned in fda but why the space from fda is similar to the reduced space from lda fda implicitly assumes the data from each group follows or approximately follows normal distribution with the same co variance matrix 29 dimension reduction algorithm fda is supervised pca is unsupervised fda can be applied on regression too what out for overt ting 111000 we can project the dim feature space into one dim space such as the projections for the blue and red so two classes are well separated lda qd in high dimension singularity of the co variance matrix when dimension is large the inverse of fork may not exist for example if pn the pp co variance matrix for lda is of rankles st hanns owe cannot compute but singularity is not serious issue and we have discussed how to singularity for lda and qd more serious issue is overt ting when dimension is large even lda could end up overt ting the data one can show that when gets large lda could behave like random guessing ie classic ation error 05 regular iz ation restrict matrices vectors to be sparse eg sparse lda or regular iz ation da or restrict features to be independent naive bayes 30 naive bayes recall for multi class problems the optimal decision rule is arg max py xx arg max kk fk require fk to be fk fk fk fk px pie each dim of is independent you can view independence as regular iz ation for high dimensional problems then each density fk jj pk is estimated separately within each class eg discrete features via histograms numerical features via kernel density estimates non parametric nb or normal densities parametric nb 31 how many parameters for parametric fk product of independent one dim normal sp means variances summary of parametric naive bayes training input xi yin output kk kj pkk make prediction input test point and output from training fork computed log 12 ex 1222 112 kp exp kp 222 kp log kp log kj xj kj 22 kj output arg mink dk 32 parameters pi by muk by sigma sq by summary discriminant analysis px yy xx px xp xxy yy joint dist marginal of conditions of xy dist of dim given qd alda fda nb parameters xd xt 1120 xt however we estimate by learning much larger collection of parameters such as 12 and next well discuss how to directly learn py xxe logistic regression tree models or directly learn the decision boundary eg sv 34
