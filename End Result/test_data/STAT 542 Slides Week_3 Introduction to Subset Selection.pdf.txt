variables election subset selection with aic bic regularization methods ridge and lasso case study boston housing data introduction to variables election in modern statistical applications nowadays we have many potential predictors ep is large and we could even have pn in some applications the key question we need to answer is to identify subset of the predictors that are most relevant to yi four goal is simply to dowell on prediction estimation ie we dont care whether the predictors employed by our linear model are really relevant toy or not then should we care about variables election to understand this lets examine the training and the test errors test vs training error training data xi yin fit linear model on the training data and trainer ky where rp is the ls estimate of the regression parameter test data xi yin is an independent data set collected at the same location is dee tester ky note that the two errors are random in the two equations above terms are colored representing sources of randomness next we decompose the expectation of the two errors into three components we can show that trainer unavoidable err bia set ester unavoidable err bias where unavoidable err we usually model fx err so even if we know we still cannot predict perfectly bias we could encounter this error if the true function is not linear or the current model misses some relevant variables notice the sign of the term which increases the tester ron average while decreases the training err so even four goal is purely prediction its not true that the more the predictors the better the prediction we should from removing some irrelevant variables
