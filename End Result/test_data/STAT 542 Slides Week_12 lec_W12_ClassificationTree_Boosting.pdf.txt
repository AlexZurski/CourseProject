how to build tree three elements where to split when to stop how to predict at each leaf node split is denoted by js split the data into two parts based on var values or not for each split goodness of split criterion seg deduction of rss for regression trees start with the root try all possible variables and all possible split values for each variable sort then values from then samples and chooses to be middle point of two adjacent values so at most possible values for and pick the best split ie the split with the best value now data are divided into the left node and right node repeat this procedure in each no def 18 stat 542 liang goodness of split js for tree we check the gain of an impurity measure js it pri trp lit where it ip pt kp tj frequency of class at node tian impurity measure pick split js that leads to large reduction of impurity measure an impurity measure is function ip pk where pj and pj pj with properties maximum occurs at the most impure node minimum occurs at pj the purest node is symmetric function of pki permutation of pj does not affect ideally we want the impurity measure to be small for each no def 18 stat 542 liang impurity measures misc rate max jp entropy deviance pk pj log pj in index xj pj pj jp specially when we have min entropy log log 11 pg in index the latter two are strictly concave consequently the corresponding goodness of split measure is always positive unless the class frequency of the left node and the one of the right node are exactly the same therefore they are often used in growing trees 18 stat 542 liang consider binary problem evaluate the following split 205100105 anode with 20 samples from class and samples from class is split into two left node has 10 samples from class and samples from class and right node has 10 samples from class and samples from class js based on rate 525102501015255150 js based on entropy 525 log 2552025 log 2520 102501525 515 log 1551015 log 1510 this split is regarded as zero gain if using but positive gain if using entropy org in which favor splits that lead to pure nodes 18 stat 542 liang boosting overview ada boost what exactly does it do the resulting will always have good prediction accuracy forward stage wise optimization for an additive model ada boost is special case of this framework with exponential loss for similarly we can develop boosting algorithms for reg with other loss functions 18 stat 542 liang ada boost consider binary problem with gx 11 he regis weak cie its performance is just slightly better than random guessing in fact its okay that is even worse than random guessing then you ll see that boosting automatically uses gx aim use combination of weak to improve the performance sequentially modify the weights on the training data fw ign sequentially pick gt output the weighted version gx sign txt tg tx the algorithm still works if gt xs are chosen randomly 18 stat 542 liang the algorithm initialize the weights ni 12 fort to taf it agt xb compute the training error rt weights wt is tx iw tii yi gt xi compute 12 log tt not et if 12 update weights wt iw tie pty ig tx iz where tis the normalizing constant to ensure that pi wt 13 output gt sign pt gt xf 18 stat 542 liang proof show that the training error measured by rate will goto not necessarily monotonically when training err gt xi ni yi sign txt gt xix ni txt it gt xi xi exp txt ty gt xii zz ty 18 stat 542 liang xi 11 exp txt ty gt xix ty ex pty gt xix iw it wt wt iz tx iw iw iw iw tiw wt wt it tty xi wt it which decreases with tift 12 since tx iw tie pty gt xix yi gt xi tie xp xi yi gt xi tie xp tex pt tex pt tr tr tt pt 18 stat 542 liang we can use agt whose error rate 12 ie worse than random guessing the nt and ada boost basically use sgt ada boost combines weak to reduce the 01 training error or more reduce an upper bound of the training error the training error of the combined gt from ada boost is not monotonically decreasing with after each iteration ada boost decreases particular upper bound of the 01 training error so in along run the training error will be pushed to zero the returned by ada boost is not guaranteed to have good performance on the test set in fact ada boost is prone to ov unless it stops early 18 stat 542 liang 10 toy example consider toy example three observations with two from class and one from class suppose at iteration we pick ah that predicts all three ob to be from class how we update their weights 123112 log 111 log 22 then exp yi xi equals or 18 stat 542 liang 11 boosting forward stage wise additive modeling consider an additive model fx txt tg tx where gt is or regression function forward stage wise optimization 02 fort to given ft choose gt to minimize xi lyi ft xi gt xi update ft ft xt gt xi boosting algorithms can take various forms depending on the choice of the base model gt the choice of the loss function ly fx and how optimization is done at 18 stat 542 liang 12 ada boost is equivalent to forward stage wise additive modeling using an exponential los sly fx exp fx arg minx lyi ft xi xi arg minx ex pyi ft xi yi xi arg minx iw tie pyi xi instead of optimizing over both and gad boost just randomly picks agt and then optimize over for any given gt denote the corresponding weighted empirical error rate by then the optimal tis given by 12 log tt 18 stat 542 liang 13 for regression we can use boosting loss function is the squared error yi ft xi xi rit xi at the th iteration ft ft txt where xt denotes the variable possibly random chosen at the th iteration and tis the estimated based on the partial residuals rit when doing the optimization at the th iteration for exponential loss the effect of the previous functions becomes weights for squared loss the effect of the previous functions becomes partial residuals for many other loss functions we dont have such simple form for the effect of the previous functions then we can approximately if xi xi by taylor expansions gradient boosting 18 stat 542 liang 14
