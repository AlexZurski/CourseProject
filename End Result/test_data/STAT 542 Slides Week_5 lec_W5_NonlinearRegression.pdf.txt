nonlinear regression models polynomial regression spline regression smoothing splines local regression generalized additive models polynomial regression assume xi ray 01 xi dx die rr create the following new variables xd xd then treat as multiple linear regression model bbbb bbb by yn cccccc can 10 bbbb bbbb 21 xd 11 22 xd 21 nx nxd cccccc cc and 10 bbbb cccc ad 11 err from now on assume is one dimensional extensions to the multidimensional case will be discussed later fit polynomial model in the following two are equivalent my xix ix my poly where poly generates design matrix with columns being the orthogonal polynomials that form basis for polynomials of degree we are more interested in the curve as well as the prediction at some new location and less interested in the estimated co because co depend on the choice of the basis function for example the two sets of co from the above commands have totally interpretation while the two curves are the same for the orthogonal polynomials the th basis function involves all the terms fork how to draw the curve in create grid of values for from small to large obtain the prediction of yon those points connect those points using command lines how to choose forward approach keep adding terms until the co for the newly added term is not backward approach start with large keep eliminating the highest order term if its not until the highest order term is instead of adding eliminating by hypothesis testing you can also run forward backward variables election with aic bic or cross validation why not search overall possible sub models question suppose we ve picked then should we test whether the other terms xj with are or not usually we dont test the of the lower order terms when we decide to use polynomial with degree by default we include all the lower order terms in our model why for example suppose the true curve is equal to ie if we polynomial model with bases xx then the optimal model is of size however we the data using poly then the optimal model would be full model with coe for all basis functions including the intercept of course if you have particular polynomial function in mind eg the data are collected to test particular physics formula constant then you should test whether you can drop the intercept and the linear term or if experts believe the relationship between and should bey 22 then you should check the output forl my xix 22 to test whether you can drop the linear term and the intercept otherwise all we care is whether the data can be by polynomial of orderd where is the highest order global vs local when the data are too wiggly we have to use high order polynomial but high order polynomials are not recommended in practice results are not stable tail behavior is bad and to interpret using polynomial functions we make global assumption on the true mean function ey xx fx but global model is less when data are wiggly recall the discussion on linear regression and kn instead we can try some local polynomial regression methods estimate the function locally using piecewise polynomials splines org it locally local regression spline models introduction tocs and nc piecewise polynomials we divide the range of into several intervals and within each interval fx is low order polynomial eg cubic or quadratic but the polynomial co change from interval to interval in addition we require overall fx is continuous up to certain derivatives regression splines smoothing splines cubic splines knots 12 mba function gona is cubic spline rt knots fig mi if is cubic polynomial in each of them intervals gx dix cix bix aix ii where and is continuous up to the nd derivative since is continuous up to the nd derivative for any point inside an interval it to check 012 ig 012 ii 10 how many free parameters we need to represent gm we need parameters dic ibi for each of them intervals but we also have constraints at each of them knots so 13 mm 411 suppose the knots fig mi are given if and are two cubic splines so is where and are two constants that is for set of given knots the corresponding cubic splines form linear space of functions with dim 412 set of basis functions for cubic splines rt knots fig mi is given by xx xx xx hi xxi 12 that is any cubic spline fx can be uniquely expressed sfx xi hj of course there are many other choices of the basis functions for example ruses the splines basis functions 13 natural cubic splines ncsa cubic spline on ab is an csi fits second and third derivatives are zero at and that is an csis linear in the two extreme intervals and mb note that the linear function in two extreme intervals are totally determined by their neighboring intervals the degree of freedom of css with knots is for curve estimation problem with data xi yin if we put knots at then data points assumed to be unique then we obtain smooth curve using nc passing through ally 14 regression splines basis expansion approach php err where pm for regression with cubic splines and pm for nc represent the model on the observed data points using matrix notation arg inky fk 215 where bbbb bbb by yn cccccc can 10 bbbb bbbb px px nh nh px cccccc can bbbb ccc cap err we can obtain the design matrix by command sbs or ns in rand then call the regression function lm 16 understand how counts the degree of feed om to generate cubic spline basis for given set of is you can use the command bs you can tell the location of knots or you can tell the df recall that cubic spline with knots has df so we need df knots by default puts knots at the mm quan tiles of 17 how counts the df is little confusing the df in command bs actually means the number of columns of the design matrix returned by bs so if the intercept is not included in the design matrix which is the default then the df in command bs is equal to the real df minus so the following three design matrices the two are of and the last one is of correspond to the same regression model with cubic splines of df bs knots quant ilex 1323 bs df bs df intercept true 18 to generate anc basis for given set of is use the command ns recall that the linear functions in the two extreme intervals are totally determined by the other cubic splines even if no data points are in the two extreme intervals ie data points are inside the two boundary knots by default puts the two boundary knots as them in and max of is you can tell the location of knots which are the interior knots recall that anc with knots has df so the df is equal to the number of interior knots plus where means the two boundary knots 19 or you can tell the df if intercept true then we need df knots otherwise we need df knots again by default puts knots at the mm quan tiles of the following three design matrices the rst two are of and the last one is of correspond to the same regression model with nc so fdf nsx knots quant ilex 1323 nsx df nsx df intercept true 20 choice of knots approach ignore the selection of locations by default the knots are located at the quan tiles of is and focus on the selection of number of knots it can be formulated as variables election problem an easier version since there are just models not pausing ic bic fold cv approach ii put large number of knots and then apply lasso or ridge but note that when number of knots goes from tok the underlying function spaces are not nested since the knots and the knots could be totally 21 summary regression splines use ls to spline model specify the df ap and then regression model with design matrix of columns including the intercept how to do it in how to select the number location of knots not the polynomial degree but the df of the spline related to the number of knots 22 smoothing splines in regression splines lets use nc we need to choose the number and the location of knots what smoothing spline start with naive solution put knots at all the observed data points xn yn fn nn instead of selecting knots lets carryout the following ridge regression will be later minh ky fk ti where the tuning parameter is often chosen by cv next well see how smoothing splines are derived from aspect 23 roughness penalty approach lets abbe the space of all smooth functions on ab among all the functions in sab look for the minimize of the following penalized residual sum of squares rss gn xi yi xi bag 00 dx where is smoothing parameter theorem ming rss ming rss where is anc with knots at then data points nw log xi xj and xn 24 log assume let gbe smooth function on band gbe anc with knots at fx ign satisfying gx ig xii first such exists since nc with knots has nd fs so we can pick then co properly such that is 25 next we want to show that bag 00 dx bag 00 dx with equality holds if and only if gg recall that rss gn xi yi xi bag 00 dx so it is easy to conclude that if holds rss gr sg that is for any smooth function we can anc sg which matches xion then samples and whose penalized residual sum of squares is not worse than the one of so theorem follows 26 proof we will use integration by parts and the fact that is anc sh xg xg note xi for nz bag 00 dx bag 00 xh 00 dx bag 00 dx bah 00 dx bag 00 xh 00 dx bag 00 xh 00 dx bag 00 dh xh xg 00 baz bah xg xd xn xi xix 12 zx xi xd xn xi xix 12 xxi xi 027 smoothing splines write xp ni ihi where his are basis functions for nc with knots atx xn xi yi xi ft where fn with fi hj xi bag 00 dx zh xii 00 xi xxi ji zh 00 ix 00 jx dx where nn with jr bah 00 ix 00 jx dx 28 or syf ty ft and the solution is arg min rss ft f1 ft yy ff f1 ft ys what if we use set of basis functions 29 de mm ler re insch 1975 basis with double orthogonality property eft fi dia gdi where is are arranged in an increasing order and in addition 10 why using this basis we have ft ft yi dia gdi ft iei 11 il si 30 smoother matrix syf ff ft ys yusin gdr basis sf dia 11 if tso columns off are the ei gen vectors of which does not depend on df of smoothing spline df trs xi 111 check the page to see what the dr basis functions look like 31 weighted spline models we can always assume all is are otherwise we just need to weighted regression model due to the following argument suppose the two ob have the same value ex where the ny gx 12 gx 122 xi yi 22 22 gx 12 222 2222 22 gx 12 so we can replace the two ob by one 22 and its weight is while the weights for other ob are 132 choice of leave one out loo cvn fold cv loo cv nn xi yi gi xi 21 nn xi yi xi ii where gi denotes the model learned based on samples ie leave the it sample out generalized cv gcv nn xi yi xi 11 ntr in you tune through df tr sp ni 111 33 simple formula for loo cv initially it seems that we need to repeatedly models when leaving out each sample but it turns out that loo prediction error can be computed based on the original model the one that uses all then samples yi ii yi yi ii where ii is the ii then try of the smoothing matrix associated with the originals smoothing spline model for simplicity we suppress the subscript in sandy denotes the prediction at xi from the originals model ie the then try of vectors solo oc vis actually much faster to compute than the general fold cv 34 in addition to smoothing splines this simple formula is true for many models such as multiple linear regression model polynomial regression model and ridge regression where the prediction at then sample points can be written syn sn ny note that is matrix computed based on is not is sometimes the following gcv is used to approximate loo cv to further reduce the computation nn xi yi ii 21 xi yi yi ii 21 xi yi yi where np is ii is the average of the trace of note that is ii trs is basically the df of linear model including the interceptor the df of ridge regression model 35 summary smoothing splines start with model with the maximum complexity nc with knots at unique points fit ridge regression model on the data if we parameter ize then cs function space by the dr basis then the design matrix is orthogonal and the corresponding coi ent is penalized no penalty for the two linear basis functions higher penalties for wigglier basis function show to do it in how to select the tuning parameter or equivalently the df what if we have collected two ob at the same location 36 local regression recall kn for regression fx xi nk xy which falls into class of curve estimates known as linear smoother sfx xi ix yi where our estimate at each is weighted average of is with ix being the weight assigned to the it sample and pi ix 137 in particular yn 10 bbbb bbbb fx fx fx cccccc cca bbbb bbbb 12 nx 11 22 nx 21 xn xn nx cccccc cca bbbb bbb by yn cccccc cca that is the value is linear transformation of note that does not depend on yy ya it doesnt mean that fx is linear function of 38 the class of linear smoother contains many curve estimators we have learned so far we will the df of linear smoot he rysy to be trs the simple formula for loo cv holds fo rysy that is yi ii yi yi ii each linear smoother usually has smoothing parameter 39 kernel smoothing issues with kn nd curve looks jagged this is because ix in xp ni ix yi is not continuous 40 kr risa kernel function satisfying dx zx dx zx dx for example gaussian kernel ex px 22 ep an ech niko kernel 341 if xj 10 otherwise 41 the local average based on kernel function can be written as xp ni xxi hy ipn xx hn xi ix yi choice of kernel not important choice of bandwidth crucial 42 local regression consider taylor expansion atx fx if xix 00 xix 02 if xie rfx xix 00 xix 02 err at each location weighted linear or polynomial regression model on yi in where zi xix then use the intercept to predict fx this prediction can still be expressed as weighted average of yi sie fx pi ix yi 43 44 loe the value of fx is obtained as follows then points those with xix smallest are called the neighborhood of nx 02 weighted least squares linear or quadratic regression fx 01 xx is in nx that is choose and to minimize xxi nx wi yi 01 xix 02 where the weights wi with ui xix max nx jx jx 45 generalized additive models gam curve estimation for one dimension xy fx err additive model for multidimensional xx pty px per fitting gam iterative ly each fi using smoothing splines or loe gam in rm gcv gam 46
