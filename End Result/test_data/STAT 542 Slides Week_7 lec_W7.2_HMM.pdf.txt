hidden markov model hmm hidden markov model hmm consider hmm for zx zn xn where is are observed and is are hidden lets assume that both zand are discrete random variables taking mz and mx possible values respectively so the hmm is parameterized by wa where mz distribution for mz mz the transition probability matrix from to bmz mx the probability matrix the emission distribution from to xt symbols in red are the elements of discrete hmm issues forward probabilities tip tz ti how uncertainty is the latent state at time given all the data till time what the likelihood of the data sequence px jx ipx nz nix in backward probabilities tip xt xn ti given the latent state at time what sour prediction on future data given xx xn how to compute them le of wa given xx xn and what can we say about the latent states ep zn jx forward probabilities the forward probabilities tip tz ti can be calculated recursively when 11 ipx ip ipx wi bix 15 when 11 px is known when 22 ipx ix px ix px jp jx px jx ix ji bix 26 the forward probabilities tip tz ti can be calculated recursively suppose tj px tz tj is known ipx xt ix px xt tj ix px tz jp jx tz tj px jx tz tj ix tj ji bix 17 backward probabilities the backward probabilities tip xt xn ti can be calculated recursively ipx nj zn ix px nz nj zn ix jp zn jj zn ipx nj zn zn aij bj xn aij bj xn nj nj for all  the backward probabilities tip xt xn ti can be calculated recursively tip xt xn ti xj px nz jj ix jp jj tip xt tip xt xn xt ti xj aij bj xt with ni 110 the baum welch algorithm the log likelihood on the observed data is given by log px ji log hx px ji which is to optimize due to the summation inside the log the log likelihood for the complete data zx is given by log az tz ny bz txt ilo gw xt log az tz nxt log bz txt 11 to describe the em aka the baum welch algorithm de ti jp ti jx tip ti xx ti at the step we have ez jx log zx ez jx log xt log az tz nxt log bz tx tim zx 11 log win xt mz xi ti log aij nxt mz xi ti log bix tmz xi 11 log wim zx ij xt ti log aij mz xi nxt ti log bix 12 mz xi 11 log wim zx ij xt ti log aij mz xi nxt ti log bix tmz xi 11 log wim zx ihm zx ij loga jim zx xxl txt lt log bil at them step when updating the parameter swab we will repeatedly use the following result which we have proved when discussing two component gaussian mixture rs consider the following function of wm wa log log am log wm where and wm is probability vector ie and pj the maximum of wis achieved by jp 013 update the maximum of pm zi 11 log wi is achieved by wi ii mz note that pi update mz mz note that each row of fai jg mz is probability vectors omz xj ij log aij is maximized by iji jp ij pn ti jp pn ti ij mz 14 update bmz mx note that each row of bf bil gm xl is probability vectors om xxl txt lt log bilis maximized by ilp txt tip tt ii zl mx in the coding assignment you will be asked not to update wi 15 how to calculate ti jp ti jx px tz ti xt xt px tz tip jj tip xt px nj ti aij bj xt 16 inference on the hidden states there are several ways to the optimal hidden state sequence given an observation sequence depending on the of optimal it yone optimal it criterion is to choose the hidden states ts that are individually most likely that is tar max ip ti jx arg max it here we can plugin from the aforementioned em algorithm such solution is optimal in the sense that it maximizes the expected number of correct states by choosing the most likely state for each however the resulting sequence may not the most likely one and it may not even be valid sequence for example and 12 but 12017 an alternative approach is to the most likely single sequence or path that is zar maxi in zn in jx the solution is obtained via dynamic programming method known as the viterbi algorithm tima xj jt jt ix which is the highest probability along single path from time to which accounts for the observations and ends in hidden state ti in particular ip ix wi bix 118 by induction we have imax jp jt tj ix max hp jt tj tpz ij tj px ii max tj ii bix note that the recursive formula above is similar to the one for ti the major de rence is that the maximization over previous states is used fort and the integration summation is used fort 19 next we solve for the most likely single sequence backward zar maxi in zn in jx the best value for zn ni stores the highest probability of az sequence that ends with zn is ozna rg max in the best value for zn note that we have already known zn jn zn arg maxi hn aij ni the best value for given we have known the optimal values for nz arg maxi ht aij ti 20
