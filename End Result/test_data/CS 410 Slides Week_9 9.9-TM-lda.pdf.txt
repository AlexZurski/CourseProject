latent dirichlet allocation lda department of computer science university of illinois at urbana champaign latent di rich let allocation lda real world observed world textdata english perceive express perspective topic mining analysis opinion mining and sentiment analysis text based prediction natural language processing text representation topic mining and analysis text based prediction natural language processing and text representation word association mining and analysis extensions of plsa plsa with prior knowledge user controlled plsa plsa as generative model latent dirichlet allocation plsa with prior knowledge users may have expectations about which topics to analyze about laptop users may have knowledge about what topics are or are not covered in document tags topics doc can only be generated using topics corresponding to the tags assigned to the document we can incorporate such knowledge as priors of plsa model maximum posterior imap estimate we may use to encode all kinds of preferences and constraints gp bp if and only if for particular doc dd 30 and 112 favors with topics that assign high probabilities to some particular words the map estimate with conjugate prior can be computed using similar em algorithm to the ml estimate with smoothing to reflect prior preferences em algorithm with conjugate prior on pwi pw pseudo counts of from prior sum of all pseudo counts what if what if battery 05 life 05 prior pw we may also set any parameter to constant including as needed deficiency of plsa not generative model heuristic workaround is possible though many parameters high complexity of models many local maxima prone to over fitting not necessarily problem for text mining only interested latent dirichlet allocation lda make plsa generative model by imposing dirichlet prior on the model parameters lda bayesian version of plsa parameters are regularized can achieve the same goal as plsa for text mining purposes topic coverage and topic word distributions can be inferred using bayesian inference 88 pl salda topic to pick topic government 03 response 02 donate 01 relief 005 help 002 city 02 new 01 orleans 005 both word distributions and topic choices are free in pl sap pk dk pw pw pw kw lda imposes prior on both likelihood functions for plsa vs ld apl salda core assumption in all topic models plsa component added by lda 10 parameter estimation and inferences in lda parameters can be estimated using ml estimator however and dj must now be computed using posterior inference computationally intractable must resort to approximate inference many different inference methods are available how many parameters in lda vs plsa 11 summary of probabilistic topic models probabilistic topic models provide general principled way of mining and analyzing topics in text with many applications basic task setup input textdata output topics proportions of these topics covered in each document plsa is the basic topic model often adequate for most applications lda improves over plsa by imposing priors theoretically more appealing practically lda and plsa perform similarly for many tasks 12 suggested readings ble communications of the acm 5547784 doi 10114521338062133826 qi oz hume xue hua shen and cheng xiang zha labeling of multi no mi al topic proceedings of ac mkd 2007 pp 490499 doi 10114512811921281246 yue lu qi oz hume and cheng xiang zha 2011 investigating task performance of probabilistic topic models an empirical study of pl and lda information retrieval 142 april 2011178203 doi 101007 107910109141913
