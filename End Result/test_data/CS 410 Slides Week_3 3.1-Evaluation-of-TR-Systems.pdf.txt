evaluation of text retrieval systems cheng xiang department of computer science university of illinois at urbana champaign course schedule big textdata small relevant data search engine recommend er system text access 11 recommendation text retrieval problem 10 web search user natural language content analysis text retrieval methods evaluation system implementation vector space model probabilistic model feedback evaluation of text retrieval systems why evaluation reason assess the actual utility of tr system measures should reflect the utility to users in real application usually done through user studies interactive revaluation reason compare different systems and methods measures only need to be correlated with the utility to actual users usually done through test collections test seti revaluation what to measure effectiveness accuracy how accurate are the search results non relevant ones efficiency how quickly can user get the result show much computing resources are needed to answer query measuring space and time overhead usability how useful is the system for real user tasks doing user studies the cranfield evaluation methodology methodology for laboratory testing of system components developed in 1960 side build reusable test collections define measures sample collection of documents simulate real document collection sample set of queries topics simulate user queries relevance judgments ideally made by users who formulated the queries ideal ranked list test collection can then be reused many times to compare different systems test collection evaluation 50 50 50 relevance judgments document collection 48 queries system system query rar which is better ra or rb how to quantify
