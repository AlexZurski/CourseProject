stat 542 11 statistical learning liang linear sv separable case first consider the scenario where the two classes of points are separable its desirable to have the width called margin between the two dashed lines to be large ie have the between the two classes as large as possible we can formulate this max margin problem as follows min 012 kk 21 subject to yi xi 010 where it xi denotes the euclid ian inner product between two vectors the constraints are imposed to make sure that the points are on the correct side of the dashed lines ie xi 01 for yi xi 01 for yi if xii son one of the dashed line siey xi 010 then we say that this it constraint or it point is active if yi xi 010 we say its inactive apparently there should be atleast two points one from each class which are active otherwise we can improve the margin this type of constrained optimization known as the convex programming problem has been well studied in the literature we actually solve its stat 542 11 statistical learning liang twin sister the so called dual problem max xi 12 iji yi yj xix subject to xi yi where the variables are and is associated with the it observation here are the connections between these two optimization problems where denote the solution for the primal problem and the solution for the dual problem complement ari ty condition if yi xi 01 that is if point is inactive ie not on the dashed line the corresponding must equal those points for which are called support vectors we can obtain an estimate of the slope after solving the dual problem based on the following equality xi yi xix ns yi xi where ns denotes the set of support vectors so pick any support vector due to the complement ari ty condition it must satisfy the equality yi xi 010 so we can solve for of course it is numerically safer to obtain an estimate of from each support vector and then take the mean of all such values for any new observation the prediction is sign xi ns yi xix note that the only depends on the value yi xi of the support vectors so its sparse solution also the is robust in the sense that if we move an inactive point around in the correct region ie dont cross the dashed line then the estimated class tay the same since the inactive points as long as they stay inactive therefore their do not the stat 542 11 statistical learning liang linear sv non separable case what if the two classes of points are not separable then we introduce as lack variable for each sample and formulate the max margin problem as follows min 01 12 kk xi subject to yi xi 01 note that only for samples that are on the wrong side of the dashed line and is automatically by the optimization set to be for samples that are on the correct side of the dashed line the optimization the between the margin kk your gain and the sum of the positive slack variables the price you need to pay and is tuning parameter which can be treated as given at this moment and is often selected by cross validation in practice similarly to the separable case we solve the dual problem max xi 12 iji yi yj xix subject to xi yi 00 the connections between the primal and the dual problems are similar to the ones for the separable case especially the inactive points ie the ones stat 542 11 statistical learning liang on the correct side of the dashed line have their so after solving for is only handful of them will be nonzero we can obtain sparse which only depends on subset of the data points ie the support vectors although this new optimization is proposed for non separable cases we can use it on separable cases too depending on the magnitude of we might be willing to pay some price to have points cross the dashed line if it will lead to bigga in of the margin stat 542 11 statistical learning liang nonlinear sv min linear vms the decision boundaries are line rie hyper planes in the feature space to obtain more that have nonlinear decision boundaries we can consider this approach embed data points into large feature space fx then operate the linear sv min the feature space since linear function of maybe nonlinear function of we can then obtain nonlinear kernel trick when operating the linear sv min the feature space the only quantity we need to evaluate is the inner product xix xxii note that did not write the inner product as xt xi since the feature space might be dimensional not dimensional euclidean space for example the mapped feature could be function and the nh xxii is just whatever inner product between two functions from the feature space apparently we have assumed that fisa hilbert space recall the linear sv solves maxi xi 12 iji yi yj xix subject to xi yi 00 for and then the prediction at new point is given by sign fx sign xi ns yi xix now the nonlinear sv solves maxi xi 12 iji jy iy jk xix subject to xi yi 00 stat 542 11 statistical learning liang for and then the prediction at new point is given by sign fx sign xi ns iy ik xix the bi variate function kin is often referred to as the reproducing kernel rk function or simply the kernel function we can view as similarity measure between and which generalizes the ordinary euclidean inner product between and popular kernels seep 434 of textbook included th degree polynomial zd radial basis the feature space is of ex px csv masa pen aliz ation method let fxx and yi 211 the nm in xi 11 yi fx ikk 26 has the same solution as the linear sv when the tuning parameter is properly chose which will depend on in so sv misa special case of the following loss penalty framework min xi lyi fx ikk the loss used in sv miscalled the hinge los sly fx fx it can be shown that kernel function must be symmetric and semi positive stat 542 11 statistical learning liang other popular loss functions for problems are negative log likelihood los sly fx log ey fx the squared error los sly xy fx 21 fx and the 01 los sly fx fy fx ga nonlinear sv associated with kernel function solves min fn xi 11 if xi fk hk where usually nonlinear belongs to hilbert space hk which is determined by the kernel function and fk hk denotes the corresponding normal though the function space hk is very large possibly dimensional beautiful result known as there presenter theorem shows that known as the reproducing kernel hilbert space rk hs stat 542 11 statistical learning liang the minimize of is always nite dimensional with maximal dim and takes the following for mar gm in kh nn xi ly if xi fk hk xx nk xx note that this is true for any loss function so were place the hinge loss in by ly fx for example suppose we use squared error loss the optimization become sky kn nk kn where kn nisan matrix with the ij then try equal tok xix and we have used the result that nk nk hk tk in other words what nonlinear sv does is to create set of new predictors xi each of which measures the similarity to the it sample and then solve for linear function of the sen predictors in the loss penalty framework where the penalty is ridge type penalty wait we have learned that ridge penalty wont lead to sparse solution but why the solution from sv miss pars eyes not like the penalty the penalty on wont lead to sparse solution the penalty in sv mis ntl but the hinge loss function is like and it is the hinge loss that leads to the sparse solution of
