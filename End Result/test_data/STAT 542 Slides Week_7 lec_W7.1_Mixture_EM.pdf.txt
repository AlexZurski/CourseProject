model based clustering model based clustering refers to clustering set of data points xn by mixture model on this data set where each cluster corresponds to component of the mixture model mixture models consider mixture model with components whose pdf is given by fx fk jk where the mixing weight is between and and pkk and fk jk is pdf with parameter random sample from the mixture model above can be generated by the following two steps generate from multi no mi al distribution with kk and 12 condition in gonz generate from fk the th component two components gaussian mixture consider simple case where xi rand each component is gaussian distribution with mean and variance iea one dimensional two component gaussian mixture model the pdf is given by px 121 1222 where 22 exp nx 222 and 122122 denotes all parameters of this mixture model given training samples xx xn the log likelihood is log px jn xi log 121 xi 1222 xii them le of the parameter 121222 is to be lear max log px which is not easy to compute why log likelihood of single normal pdf takes derivative friendly form log 12 log 222 const but log likelihood of weighted summation of normal pdfs does not the calculation is much easier if we knew which component xi belongs to introduce the latent variable zi or zi bern xi zi nk the likelihood of the full data is given by nyi 121 xi if zi gh 1222 xi if zi the log likelihood is given by xi zi log 121 xi log zi log 222 xi log xi zi log 121 xi log xi zi log 222 xi log 16 them le for 12212 is given by 11 xi zi xi 211 xi zi xi 1221 xi zi xi 221 xi zi xi 22 and why them le of isn nn log nn log log log log log 11 nc where is constant not depending on and the sum is the negative kl distance between two distributions which is non positive and is zero only if kull back leib ler distance the kl distance between two distributions and is to be px log px qxd xor mx pj log pj for continuous and discrete cases respectively note that kl distance is not symmetric using jensen inequality we can show that kl pk qep log px qxe px log qx px loge px qx px so kl pk and and are the same distribution up to measure zero set however we do not observe is consider the following iterative scheme start with some initial guess of then calculate the corresponding distribution of zip zi xii 121 xi 121 xi 1222 ip zi xi now for each point xi instead of allocating it to component or we count its fraction to component and fraction to component and update 121222 as follows 11 xii xi 211 xii xi 1221 xi xi 221 xi xi 22 we can iterative the two steps until the value of gets stabilized is the returned value of them le that maximizes the marginal likelihood px 10 the em algorithm the expectation maximization em algorithm is an iterative method that them le by enlarging the sample with unobserved latent data suppose our observed data is with log likelihood log px that depends on unknown parameter using latent variable the log likelihood can be written as log px log px log zp px direct maximization of is quite due to the sum inside the logarithm in the em algorithm we pretend we knew then we can maximize log of the joint likelihood log px log log px 11 each iteration of the em algorithm involves two steps the step and them step step let denote the current value of find jx the distribution of the latent variable given the data and and then calculate the following expectation ez jx log px which is zp zz jx log px or zp jx log px jd zm step find that maximizes replace by and repeat the above and steps until convergence 12 next we show that px px that is each iteration of the em algorithm increases or atleast doesnt decrease the marginal likelihood px recall ez jx log px jg ez jx log px px ez jx log px jx px jx log px px ez jx log jx jx where the nd term is the kull back leib ler distance between two distributions which is always non negative solo gp xj px ez jx log jx jx 013 an alternative view of em the em algorithm is essentially an mm algorithm neal and hinton 1998 consider the following objective function qe log px where denotes any pdf pm of zand eq denotes an expectation of taken with respect of the objective function can be re expressed as qe log px jp jx log px eq log zp jx so max is achieved by setting le and to be xml in other words we can obtain leas byproduct of maximizing 14 next we show that em can be viewed as coordinate descent algorithm on qat the th iteration step qt arg max tpz jx tm step arg max qt arg max he xt log px ji the alternative view of em provides for some variants of em algorithms such as generalized em gem where only partial implementation of the or steps is performed can handle cases where we have some special constraints on the latent variable gra cae tal 2007 motivates variation alem algorithms 15 variation alem given the optimal choice for isp jx which maximizes qe log px jp jx but jx may not be easy to obtain and approximation is needed for tractable computation for example we can optimize subject to constraint that can be factorize da sq ni qi zi then we can apply coordinate descent over to maximize ne log px jp jx nz 16 latent dirichlet allocation lda 17 source ble 2012 probabilistic topic models 18
