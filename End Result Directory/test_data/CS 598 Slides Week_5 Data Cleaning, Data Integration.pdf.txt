  data cleaning data integration university of illinois at urbana champaign data cleaning data integration data cleaning data integration heterogeneity federation vs derived combinations data cleaning data cleaning is general term used colloquially to describe preparing data for analysis when single schema is involved the phrase typically suggests dealing with duplicate records values that are missing out of bounds or inconsistent data typing errors or inconsistencies data entry errors attribute interpretation errors variation in null handling misapplication of standards inadequate normalization missing or inadequate schemas missing relationships failures of referential integrity or other constraints failure to match schema egas identified by formal grammar or xml parser data cleaning is profoundly important without it data cannot be used reliably or at all it also is major industry expense and consume much staff time when multiple schemas or disparate instances are involved the data preparation task is data integration data integration data integration combining data residing indifference sources and providing users with unified view lenz erin 2002 data model relationships relations eg relational databases schemas column and key descriptions trees eg xml documents schemas grammar seg dtd entities relationships conceptual models uml or er models on to logie triple seg rdf triple stores schemas serialization descriptions conceptual level physical level or storage logical level files records delimiters data structures indexes etc schema serum schemas rdf owl why is it important real world problems are profoundly interdisciplinary solving them requires integrating diverse data from multiple sources egan effective response to an impending natural disaster can require understanding how many people will be affected hospital location and capacity and transportation routes and soon many different disparate databases will need to be accessed demographic meteorological geographical and most importantly the data elements will need to be related concentrations of people connected to transportation routes the storm path hospital capacity etc if this cannot happen or cannot happen reliably or efficiently much valuable data will be useless opportunities lost and problems unaddressed why is it hard when datasets are developed by different communities and for specific purposes integrating them with other datasets is often not anticipated and accommodation would be hard in any case these datasets often use different data models schemas and encodings that are very hard to related to each other even when describing the same real world feature but unless common concepts can be found to connect data across datasets and to either standardize or re factor related data elements integration is impossible the obstacle to data integration is therefore heterogeneity kinds of heterogeneity encoding heterogeneity different mappings from bitstream into bytes characters numbers or other logical units syntax heterogeneity different data description languages for the same model type eg rdf xml vs model heterogeneity different model type eg relations vs entities relationships representational heterogeneity different modeling choices within model type eg relationships vs entities semantic heterogeneity different conceptualization of similar domain features processing heterogeneity eg different maintenance and update regimes policy heterogeneity eg different privacy and security rules varying ownership and licensing etc adapted from bertram lu dsc her kaili shawn bowers efrat jae ger frank boy an broda ric cha it an baru managing scientific data from data integration to scientific work flows geo informatics data to knowledge 2006 and amit she th changing focus on interoperability in information systems from system syntax structure to semantics in good child me gen hofer rf ge as and ck ott man editors inter operating geographic information systems kluwe 1998 relatively easy often difficult usually very difficult two general approaches to integration federation vs derivation federation for relevance standardized metadata attached to each data set can be used to determine its relevance indicating spatial and temporal location and general nature of content this facilitates discovery for queries views on and queries against multiple databases are supported by mappings to mediating meta schema derivation single data set is derived from multiple sources and governed by single schema compare extract transform and load et data warehouses in either case heterogeneity remains huge challenge 
