simulation kn nvs linear regression review two simple approaches for supervised learning nearest neighbors kn and linear regression then examine their performance on two simulated experiments to highlight the trade between bias and variance nearest neighbors xxi nk xy where nk denotes samples from the training data which in terms of their values are closest to regression kn predicts by local average kn can return the majority vote in nk xe gy if kp xi nk xy 05 assuming 10 gor return probability vector calculated based on the frequencies in nk what are the input parameters for kn none input parameter is the neighborhood size for nk the prediction at xi the it training sample is exactly yi ie zero training error for nn nk every neighborhood contains all then training samples so the prediction is the same no matter what value takes the complexity or the dimension of kn is roughly equal tonk no magic value fork it is tuning parameter of the algorithm and is usually chosen by cross validation if you dont know what cross validation is read chap 51 in is lr the other input parameter is the metric which we use to the neighborhood the default is the euclidean distance on the dimension feature vector rp however it could be the weighted euclidean eg xx px xj xj and we would like to learn the weights js from the data it does not need to be euclidean as long as it is similarity measure for any two samples eg in image from flickr we can measure the similarity of two images by their physical similarity or by the similarity of their tags or by the percentage of people who like both images linear regression in linear regression models we approximate by linear function of fx 11 xp and estimate js using the so called least square sls principle min pn xi yi xi 11 xi pp the solution is easy to compute call command lm in we can also apply linear regression on problems with 01 and predict to be if the ls prediction fx is bigger than 05 and otherwise there are some drawbacks with ls for the squared if xi is not good evaluation metric for ideally we would like to estimate the py xx however the linear function fx could return us values outside 01 later well learn generalization of ls called logistic regression model where we assume the log it of the probability is linear function log px px 11 xp despite some of the drawbacks the ls approach for works reasonably well in practice plus its computation is very fast so well apply lson the two toy examples
