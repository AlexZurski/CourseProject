probabilistic topic models mixture model estimation department of computer science university of illinois at urbana champaign probabilistic topic models mixture model estimation real world observed world textdata english perceive express perspective topic mining analysis opinion mining and sentiment analysis text based prediction natural language processing text representation topic mining and analysis text based prediction natural language processing and text representation word association mining and analysis back to factoring out background words text 004 mining 0035 association 003 clustering 0005 the 0000001 the 003 002 is 0015 we 001 food 0003 text 0000006 bpd 05 pb 05 topic choice pd text mining clustering text the text mining paper estimation of one topic pw text mining association clustering the the 003 002 is 0015 we 001 food 0003 text 0000006 bpd 05 pb 05 topic choice pd text mining clustering text the adjust to maximize pd all other parameters are known would the ml estimated emote background words in behavior of mixture model text the likelihood ddp bb 0501 0509 text the the 09 text 01 bpd 05 pb 05 pd 0501 05 pd 0509 how can we set pd to maximize it note that pd and text the pd text the the 09 text 01 bpd 05 pb 05 0501 05 pd 0509 note that pd if then reaches maximum when 05 pd 050105 pd 0509 pd 09 pd 01 behavior if pw pw then pw pw response to data frequency text the dp dd 0501 05 pd 0509 pd 09 pd 01 text the the the the dd 0509 0501 05 pd 0509 0509 0509 pd 01 or 01 behavior high frequency words get higher pw what if we increase pb summary general behavior of mixture model every component model attempts to assign high probabilities to collaboration competition between the component models fixing one component to background word distribution ie background language model his an example of imposing prior on the model parameters prior one model must be exactly the same as the background lm
