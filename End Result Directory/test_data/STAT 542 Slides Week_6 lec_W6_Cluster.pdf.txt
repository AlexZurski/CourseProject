clustering goal group objects into subsets or clusters such that objects within each cluster are more similar to one another than objects assigned to clusters choice of distance measures dx is crucial as metric in mathematics distance measured za dx dx if and only if zd zd zx xy dx zd zy triangle inequality it is okay to use distance measure that does not satisfy all the properties dissimilarity measures euclidean distances dx zl px xj 212 px xj jj manhattan distance max pj xj jj lim px xj jd non euclidean distances jac card distance between sets jab jab fac deg and fade is 134 eg can measure the distance between sentences cluster analysis arranges similar objects in the same group cluster analysis divides data into groups or distance between movies restaurants based on their ratings hamming distance between two strings of the same length number of positions at which the corresponding symbols are rent karol in and kath in is 31011101 and 1001001 is for binary strings the hamming distance is the same asl distance eg can measure distance between texts dna protein sequences edit distance number of inserts and deletes to change one string into another eg abcde andy bc duve is cosine distance angle between two vectors andy arc cos ty ky two types of inputs for clustering data matrix np where rows stand for objects samples and columns stand for variables features dissimilarity matrix nn where di jd ji measures the between and we can transform the input from one form to the other xd easy with given distance measured various choices eg multidimensional scaling mds classical multidimensional scaling cm ds given pairwise squared distance matrix dn ndi where di ppl il xj can we retrieve then data points is up to location shift sign and ortho normal rotation double centering to obtain dd ij di did jd where ii throw me and jj th column me and overall mean of sv dpc of dn nun pdp put the nx np ud 127 key idea double centering dd xn px tp ndi jk xix jk ti xix tj xj xt ix assume pix and write np ni xi di nn xj di xt ix icd nn xi di jc xt jx jd nn xi di cdi di did jd 22 xt ix xt ix instead of using all dimensions of from the sv of we can just extract the dimensions oz zn rk for mak dimensional representation of the data that approximates the pairwise distances given by check command cm scale there are some variations of cm ds check iso mds for kruskal non metric and mds and ammon for ammons nonlinear mapping in package mass means clustering input np data matrix and number of clusters means clustering algorithm start with some initial guess for the cluster centers for each data point the closest cluster center partition step replace each center by the average of data points in its partition update centers repeat 12 until convergence 10 goal partition data into groups so the within cluster dissimilarity is small dissimilarity measure is the squared euclidean distance optimize the following objective function within cluster sum of squares nm kn xi xi mz ik xi ikk xi mk where zi 12 and mk rp 11 means converges to local minimum of by iterating the following two steps step update partition given the cluster centers mk zi arg mink kk xi mk step update centers given the partition zn karg min mx iz ikk xi mk mean off xi zi kg 12 some practical issues try many random starting centers and choose the solution with the smallest within clusters check the option start ink means in dimension reduction via pca or random project if using other dissimilarity measure show should we modify the means algorithm you just need to change step 24 how to choose 13 dimension reduction fork means the computation cost fork means iso in pki iterations points features centers apply dimension reduction techniques to reduce pp cause the top directions to form the best approximation of the pairwise distance on average random projection or johnson linden strauss type embedding generate random gaussian matrix rd where do log project xi to zi xi the nw hp for any and kz iz jk xix jk 21 kz iz jk 214 means with other distance measures to local minimum of nm kk xi zi xi mk iterates the following two steps step update partition zi arg mink xi mk step update centers karg min piz ik xi meg xx where is categorical variable taking values fab cg and xy 04 jx 06 ix 26 215 how to update centers at step suppose cluster contains the 10 objects then the st dim of is the median of the st dim of the 10 objects mina 10 xi the nd dim of is the most frequent value among abc gmina abc gp 10 xi 26 ay with step what about edit distance sometimes you ll that the computation is easier if were strict the centers mk to sample points ie the centers are representative points from the sample further all we need isdn and no need for data matrix np 16 km edo ids clustering input nn pairwise dissimilarity matrix and cluster centers medo ids are restricted to be data points the center for cluster km karg minx iz ik xj jkd ij is set to be one of the data points in cluster such that the sum of its pairwise distance to all the other points in that cluster is the smallest 17 partition around medo ids pampa kaufman and rous see ew 1990 minimizes the following objective function min nm kk xi zi xi mk where each mk is one of the data points by iterating the following two steps partition zi arg min kk xi mk update medo ids based on 118 in addition pam adds swapping step at the end to avoid local minimal it keeps swapping xix that decreases the objective function the most where xi fm kg xj fm kg until convergence 19 how many clusters in supervised learning the target is predict well but in unsupervised learning there is no so it is not clear how to evaluate the of an unsupervised procedure such as clustering methods for selecting gap statistics silhouettes statistics prediction strength 20 gap statistics many measures of goodness for clustering are based on the tightness of clusters eg the within clusters skk ikk xi mk gap statistic tib shi rani walther and has tie 2001 ke logs ski logs sobs bb xb logs sbk logs sobs where sobs is the within clusters computed based on the observed data and sbk is computed based on fake data generated from the reference distribution which has no clustering structure 21 how to generate data from the reference distribution two choices suggested by tib hira niet al 2001 generate each feature uniformly over the range of the observed values for that feature original data np generate referenced tazz zp by uniformly sampling from the range of the th column of xb generate features from uniform distribution over the ranges of the principal components of the data if xn pu dv tx np xv ud and then sample features uniformly over the ranges of the columns of as in and then transform back to get the referenced tazz vt 22 one standard error se rule kop tar gm in kf kg kg sk where sk sd logs kp 11 starting from we would like to pick the whose performance is better than the one of eg kg since the gap statistics gare computed based on samples to take account of the stochastic uncertainty we think any value from this interval sk can represent the performance of which leads to the se rule 23 silhouettes statistics the silhouette statistic rous see uw 1987 of the it hobs measure show well it in its own cluster versus how well it in its next closest cluster adapted tok means aik xi mk ik xi lk where ck and cl is the next closest cluster to xi then its silhouette is sibi imax fai big 24 si fork means si but can be below for other clustering methods si object ii swells object lies intermediate between two clusters si object is badly the larger the average silhouette the better the cluster 25 silhouette cos average silhouette widths rule of thumb from an jae tal clustering in an object oriented environments 70 strong structure has been found sc 50 reasonable structure has been found sc 26 the structure is weak and could be try additional methods sc 26 no substantial structure has been found we can plots ck over and pick that achieves the largest sc 26 prediction strength step divide the data into and two sets training and test step cluster the mobs in into clusters ck of size mk the truth step cluster into clusters and use the corresponding clustering rule to assign the mobs from into clusters the prediction 27 can we evaluate the accuracy on the mobs in with the label being their cluster membership not appropriate because of the labelling issue the meaning of cluster changes when data change cluster at step is not the same as cluster at step better choice is to measure the error on the association or co membership matrix aij if and are in the same cluster and otherwise 28 step divide the data into and two sets training and test step cluster the mobs in into clusters ck of size mk the truth step cluster into clusters and use the corresponding clustering rule to assign the mobs from into clusters the prediction prediction strength tib shi rani and walther 2005 measures the worst performance of predicting pairwise co membership min kg nj nj ii 02 cj iii where iii 01 if and assigned to the same group at step 229 hierarchical clustering methods input pairwise dissimilarity matrix of observations rule to calculate dissimilarity between disjoint groups of observations output hierarchical clustering result single point clusters at the lowest level one cluster at the highest level clusters at one level are created by splitting merging clusters at the next higher lower level 30 bottom up clustering starts with all observations separate and successively joins together the closest groups of observations until all observations are in single group dissimilar ty or distance between group so fobs single linkage nearest neighbor complete linkage furthest neighbor group average 31 top down clustering starts with all observations together and successively divide into two groups until all observations separate hybrid chip man and tib shi rani 2006 check the hybrid hc lust package 32
