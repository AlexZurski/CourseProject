networks graphs graph contains vertex set vf vi gni just refer to no devi as node ian edge set wwi if viand vj are connected eiji in the edge set wi may take any positive real value indicating the strength of the connection think of was an nn symmetric matrix  similarity graphs vf xi gni fw jg wi similarity xix ex wi correlation or exp xix jk 222 ex neighborhood graph wi if xix ex kn graph wi if neither in or is among the other skn nina mutual kn graph wi if atleast one of ij is not among the other skn graph partition consider bi partitions va what is good partition how to such partition clustering objectives goodness of clustering points in the same cluster should be similar points in nt clusters should be dissimilar in the similarity graph representation we want to maximize within cluster weights minimize between cluster weights min cut objective function cuts ab xi xj wi partition membership indicator qi bk objective function cut 18 xi wi iq 214 td wq approximate by the nde gen vector of dw qd dia gw la plac ian matrix of graph ld lis symmetric and semi positive 12 xi wi ji 201 tei gen vector is with 102 nde gen vector is the desired solution lets exclude the trivial case where lhasa block diagonal form higher ei gen vectors can also be used for further partition recovering the partition choose the splitting point for split at afi qi gb fi qi split at median if requires jjb balance sizes choose the optimal splitting point to minimizes ab drawback of min cut normalized cut normalized cut shi malik 1997 jn cuts abs vs bas saas vs bsb vva sav sum of weights on edges originating from set volume min jn cut is equivalent to minimize normalized between cluster weights maximize normalized within cluster weights partition membership indicator irv va vi aq irv vb vi where vs td td 10 objective function jn cut xi wi iq 212 td wq solve by the generalized eigenvalue problem nde gen vector dw qq or id wq 10 multi partition recursive bi partition use multi partition objective functions spectral clustering embed data in subspace of ei gen vectors cluster embedded data points using another algorithm such ask means 11 spectral clustering dia wil id where lis known as the normalized graph la plac anthem non constant ei gen vectors corresponding to the lowest eigenvalues of form new data matrix nk mie each node corresponds to data point in rm run means based on the data matrix 12 next we justify the use of spectral clustering via the so called stochastic block model stochastic block model probabilistic model for graphs assumes grouping structure among the nodes we can show that if we run spectral clustering on the corresponding matrix we can correctly retrieve the underlying grouping structure 13 statistical models for graphs networks nodes etv is given wis stochastic binary matrix the simplest one parameter model aka erdo re nyi model pwi the most complicated many parameters model pwi wi something in the middle cluster wi js 14 stochastic block model holland et al 1983 suppose nodes are from groups block sbk connecting probability between nodes from the blocks conditioning on node in th block and node in th block pwi wi jbl 15 block cluster membership zi 201 kno dei in th block zi except il stack all the is sank matrix then the stochastic block model implies nz nk bz teach row of has one and only one value equal to and all others 016 spectral clustering at the population level assume bz tis known the population la plac ian ldi ix kwik ld 12 wd 12 it can be shown that the top kei gen vectors of are equal to where is kk matrix therefore spectral clustering can retrieve the clusters correctly lemma 31 from spectral clustering and the high dimensional stochastic block model by karl rohe sour av chatterjee bin yu 17 show that can be re expressed as lz lzt where db dia bz rk kb ld 12 bb 12 assume tz 12 lzt 12 has the following sv dz tz 12 lzt 12 vt left multiply zz tz 12 and right multiply its transpose on both sides zb lzt zz where tz 12 note that tzi ks oz are the ei gen vectors 18
