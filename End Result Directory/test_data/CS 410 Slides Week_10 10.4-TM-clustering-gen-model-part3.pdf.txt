text clustering generative probabilistic models part department of computer science university of illinois at urbana champaign text clustering generative probabilistic models part 32 real world observed world textdata english perceive express perspective topic mining analysis opinion mining and sentiment analysis text based prediction natural language processing text representation topic mining and analysis text based prediction natural language processing and text representation word association mining and analysis how can we compute the ml estimated at collection of documents cd dn model mixture of kuni gram lms ip ii to generate document first choose according topi and then generate all words in the document using pwi likelihood maximum likelihood estimate em algorithm for document clustering initialization randomly set ip ii repeat until likelihood pc converges step infer which distribution has been used to generate document hidden variable zd km step re estimation of all parameters an example of clusters random initialization step hidden variables zd 12 205 pw pw text 0501 mining 0201 medical 02075 health 01005 cw text mining medical health document normalization to avoid underflow pw pw text 050105012 mining 020102012 medical 02075020752 health 01005010052 average of pwi as possible normalize an example of clusters cont step from este pp pw pw text mining medical health zd dd 109 201 308 123 212 343 summary of generative model for clustering as light variation of topic model can be used for clustering documents each cluster is represented by unigram lmp wi term cluster document is generated by first choosing unigram lm and then generating all words in the document using this single lm estimated model parameters give both topic characterization of each cluster and probabilistic assignment of document into each cluster corresponding to the unigram lm most likely used to generate the document em algorithm can be used to compute the ml estimate normalization is often needed to avoid underflow
