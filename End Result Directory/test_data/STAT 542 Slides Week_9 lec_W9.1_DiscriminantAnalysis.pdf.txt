we haven observations each of them consists of measurements and they are from two classes the goal is to rule which takes the measurements as the input and outputs the class label we would like our prediction rule makes small errors not only on then samples but also on future observations project lending club data project sentiment analysis lets focus on binary at this moment how to learn collect training sample xi yin where xi pyi 01 pick collection of functions frp 01 pick loss function eg measure the performance of at xy by loss function lf xy eg the 01 loss lf xy if fx if fx find an optimal by minimizing min nn xi lf xi yi the optimal consider an ideal situation we have samples or equivalently we knowhow data xy are generated then nn xi lf xi yi ex yl xy where the expectation is taken rt the true data generating process px risk fe xy lf xy the optimal is given by far gm in frisk suppose we adopt the 01 loss and are allowed to use any function what the optimal risk fe xy lf xyz zy ly xp xy dy dx zx zy ly fx py jx px dy dx zx hz ly fx py jx dy ipx dx where px is the marginal distribution function of xp yj is the conditional distribution function of given xx px py jx is the joint distribution function of xy risk zx hz ly fx py jx dy ipx dx the problem off that minimizes risk reduced to series of sub problems for each given the optimal value of fx to minimize the inside integral fx ar gmina zy lya py jx dy we can do this for every and then the resulting of course it may not be continuous minimizes risk turns out to be not to solve note that is just discrete random variable taking two possible values whose pm is given by py xx py jx given the integral over yas function of is given by zy lya py xd yl py xl py xl axl 81 if if of arg min fr 81 if 050 if 05 the optimal is called the bayes rule and the corresponding risk rf is referred to as the bayes risk or bayes error which is what you computed in coding assignment 16 the calculation can be easily extended to multi class for multi class problems where kg the optimal rule is fx arg max py kj xx the that output 01 divide the space into regions and each region is assigned to either or sometimes we do not describe but the decision boundary linear ers refer to methods whose decision boundaries are linear function of discriminant analysis estimate the joint distribution px px jyp by estimating px jyd is to fx in each class and py marginal frequency of each class and then use bayes theorem to things around and obtain py jx quadratic discriminant analysis qda linear discriminant analysis lda and its connection with fisher discriminant analysis fda naive bayes bayes theorem for we have learned that the optimal ie the bayes rule does based on the conditional probability which by the bayes theorem takes the following form py kj xx xxy px xp xx jy py kp xx fk xp xx fk fk denotes px jy and py fk conditional density function of xj ykk py the marginal probability or prior probability for class we can write the decision function sfx arg max kk fk arg max kh log log fk xi to construct we just need to estimate ks and fk 10 qd given lets model the dim feature by multivariate normal distribution with mean and co variance matrix kk bbbb bbbb kp cccccc cap 11 bbbb 11 pk pp cccc app its pdf is given by fk jk 12 ex ph 12 kt ix kt px px kj xj kj xl kl 11 py kj fk xe dk dk log fk log ix kt log jk log where the term is the so called mahal nobis distance between and and py is the class frequency for the th class we can predict to class if dk achieves the minimum among dk 12 the rule above is called quadratic discriminant analysis qd since it leads to quadratic decision boundaries in practice we need to estimate kkk sample frequency and mean for each class nk nk nk xi yi xi sample co variance matrix for each class nk xi yi xi kp xi kt what if does not exist replace it by kip where is small number eg 00113 summary of qd training input xi yin output kkk kk see formulae on previous slide make prediction input test point and output from training fork computed xx kt log jk log output arg mink dk 14 lda if further assume all we need to compute is dk xx kt log jj log which is linear function of xx kt xt xt kt estimate by the pooled sample co variance matrix nkk xi yi xi xi kt 15 what if does not exist replace it by ip where is small number or compute as follows assume up 30 bbbb 1000 200001 cccc aut bbbb 10001 200001 cccc aut 16 reduced rank lda suppose is an identity matrix ip then we can write the discriminant function for lda as dk kk 22 log the feature vector only appears in the st term which is the squared distance from tok the center of the th class two points determine line three points determine plane class centers determine ak dim subspace next well show that we can replace the squared distance kk in the original dim space by square distance in ak dim subspace that is lda naturally leads to dimension reduction from tok of course here we assume 17 log assume we care about the relative distance among points so we can always move the origin to without the distance the vectors for mak dim subspace in rp denote this subspace by for any vector in rp xx 12 22 where is the projection of on to the dim subspace and is the projection of on to the pk dim subspace that is orthogonal to how to get this decomposition you can run regression of against pk and hx the vector the residual vector where bb tb bt 18 due to the orthogonality of and we have tx xt xt xt 22 the norm square length square of vector norm square of its projection on to norm square of its projection on to an owlets look at the square distance from point tok the center of the th class kk 20 kk ck where the constant ck is the same for all note that the projection of kon to is zero 19 note that for all we care is the magnitude of dk dk kk 22 log kk kk 22 log const so we can operate lda on this reduced dim space replaced tax by its projection on to and computed kk 22 log which is the same as running lda in the original dimensional space we assume in our derivation 20 suppose is not identity assume its sv da sud ut where is diagonal with non negative entries and is an ortho normal ie rotation matrix with uut ip then we transform to xx px 12 rp 12 ud uta then the co variance matrix of is identity cov cov 12 12 cov 12 tud tud tud tud tud uti paho wwe scale dim variable to have unit variance we multiple that variable by you can view 12 as the inverse of the square root of in the multidimensional setting note 12 is symmetric and 121221 summary of reduced rank lda training input xi yin output kkk and 22 make prediction input test point and output from training computes vd of ud ut and aud ut compute the projection matrix based on akb pk ha ak ih bb tb dimension reduction xh fork computed kk 22 log where kaki the mean of the th class in the reduced space output arg mink dk 23 when pk this means that for lda one can project the data on to lower dimensional subspace dim eg just one dimension for binary at ionas we will see that the same subspace also arises in dimension reduction method called fisher discriminant analysis fda though fda is motivated from slightly aspect caution each of the directions are linear combinations of the original dimensions and the weights are all learned from the data so can occur good separation on just the or even less directions on the training data but the same good result cannot be reproduced on the test data see the analysis of the digits data on the page 24 fishers discriminant analysis find direction rp such that the projection of data on to this direction is well separated denote the projection of an observation xi rp by ui at xi what being well separated the group means of is are far apart from each other and within each group the variation spread is small ie maximize the following ratio between group variation within group variation at baa twa 25 the within class and between class sample co variance matrices wp nkk xi yi xi xi kt pp nkk kt the following qualities are trivial nx ny ny you can view was the sample co variance matrix over xi yi same as the pooled sample co variance matrix in lda and bas the sample co variance matrix over the class centers yi 26 generalized eigenvalue problem max at baa twa max at ba sub to twa assume wu ut 12 where 12 ud ut is symmetric and write its inverse as 12 ud uta tba at 12 12 12 12 aw 12 at 12 12 12 btw 12 12 sub to kb 21 the optimization above is classical eigenvalue problem 27 we can solve the directions sequentially as follows the tei gen vector of matrix 12 12 then solve 12 the nde gen vector of matrix 12 12 then solve 12 note that although bj are ortho normal but js are not bt bj at wa bt jbl at wal we can extract at most directions since the rank of is the directions span exactly the same space as the one from the reduced rank lda 28 lda vs fd alda fda dimension reduction method ie the output of fda is set of directions but not rule the normal assumption is never mentioned in fda but why the space from fda is similar to the reduced space from lda fda implicitly assumes the data from each group follows or approximately follows normal distribution with the same covariance matrix 29 lda qd in high dimension singularity of the co variance matrix when dimension is large the inverse of fork may not exist for example if pn the pp co variance matrix for lda is of rankles st hanns owe cannot compute but singularity is not serious issue and we have discussed how to singularity for lda and qd more serious issue is when dimension is large even lda could end up the data one can show that when gets large lda could behave like random guessing ie error 05 regular iz ation restrict matrices vectors to be sparse eg sparse lda or regular iz ation da or restrict features to be independent naive bayes 30 naive bayes recall for multi class problems the optimal decision rule is arg max py kj xx arg max kk fk require fk to be fk fk fk fk px pie each dim of is independent you can view independence as regular iz ation for high dimensional problems then each density fk jj pk is estimated separately within each class eg discrete features via histograms numerical features via kernel density estimates non parametric nb or normal densities parametric nb 31 summary of parametric naive bayes training input xi yin output kk kj pkk make prediction input test point and output from training fork computed log hk ex 1222 11 kp exp kp 222 pi log px log kj xj kj 22 kj output arg mink dk 32 summary for the ultimate goal is to estimate py kj xx in discriminant analysis da we estimate the joint xxy kp xx jy py and then obtain py kj xx dais conceptually simple and works for some low dimensional problems but not an way of building for example for binary lda with discriminant function dk xt kt log what matters is the decision boundary which is linear function has 33 parameters xd xt 1120 xt however we estimate by learning much larger collection of parameters such as 12 and next well discuss how to directly learn py kj xxe logistic regression tree models or directly learn the decision boundary eg sv 34
